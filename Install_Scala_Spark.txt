Install Java

# Check to see if Java is already installed.
    ---java -version
# Create repository for java.
    ---sudo add-apt-repository ppa:webupd8team/java
# Update packages.
    ---sudo apt-get update
# Install Oracle JDK by using following command. After running following cmd it will pop-up window. Enter OK button and aggre the licence.
    ---sudo apt-get install oracle-java8-installer
# Verify java installed.
    ---java -version
    ---whereis java


Install Scala

# Run below command:
    ---sudo apt-get install scala
# Type scala into your terminal:
    ---scala
# You should see the scala REPL running. Test it with:
    ---println("Hello World!")
# You can then quit the Scala REPL with:
    ---:q


Install Spark

# For installing spark we need git for this.
    ---sudo apt-get install git
# Now, go to https://spark.apache.org/downloads.html and download a pre-built for Hadoop 2.7 version of Spark. Then download the .tgz file and save it on your computer. You will find this tgz file into your Downloads floder and extract it.
    ---cd Downloads
    ---*****@ubuntu:~$ cd Downloads
    ---*****@ubuntu:~/Downloads$ ls -ltr
    ---*****@ubuntu:~/Downloads$ tar xvf spark-2.0.2-bin-hadoop2.7.tgz
# Then once its done extracting the Spark folder, use:
    ---*****@ubuntu:~/Downloads$ cd spark-2.0.2-bin-hadoop2.7
    ---*****@ubuntu:~/Downloads/spark-2.2.0-bin-hadoop2.7$ cd bin
    ---*****@ubuntu:~/Downloads/spark-2.2.0-bin-hadoop2.7/bin$ ./spark-shell (and you should see the spark shell pop up)
# Write a simple statment to check spark is working. (scala)
    ---scala> println("Hi, How are you?")
              Hi, How are you?
# Run pyspark.
    ---*****@ubuntu:~/Downloads/spark-2.2.0-bin-hadoop2.7/bin$ ./pyspark (and you should see the spark shell pop up)
# Write a simple statment to check spark is working. (Python)
    --->>> print("Hi, How are you?")
           Hi, How are you?

Error sqlContext
# Check sc and sqlContext working or not.
    ---scala> sc
       res1: org.apache.spark.SparkContext = org.apache.spark.SparkContext@16269ff4
    ---scala> sqlContext
       <console>:23: error: not found: value sqlContext
# Solution for sqlContext. Run below command.
    ---scala> val sqlContext = new org.apache.spark.sql.SQLContext(sc)
       warning: there was one deprecation warning; re-run with -deprecation for details
       sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@3eadfbbb
# Check sqlContext working or not.
    ---scala> sqlContext
       res2: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@3eadfbbb       